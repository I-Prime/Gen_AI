# -*- coding: utf-8 -*-
"""Graph_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ot2KXDYDt3bALSimIOKXpitY1artenau
"""

!pip install langchain-graph-retriever

!pip install graph_rag_example_helpers

!pip install langchain_openai

from graph_rag_example_helpers.datasets.animals import fetch_documents

animals=fetch_documents()

animals

len(animals)

animals[1].page_content

animals[0].metadata

animals[0].page_content

animals[1].metadata

!pip  install langchain_huggingface

from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

embeddings.embed_query(animals[0].page_content)

from langchain_core.vectorstores import InMemoryVectorStore

vector_store=InMemoryVectorStore.from_documents(
    documents=animals,
    embedding=embeddings
)

from graph_retriever.strategies import Eager

from langchain_graph_retriever import GraphRetriever

traversal_retriever = GraphRetriever(
    store = vector_store,
    edges = [("habitat", "habitat"), ("origin", "origin")],
    strategy = Eager(k=5, start_k=1, max_depth=2),
)

traversal_retriever

results=traversal_retriever.invoke("what animal could be found near a anaconda?")

len(results)

results[0]

pip install -U langchain-groq

from langchain.chat_models import init_chat_model

import getpass
import os

if not os.environ.get("GROQ_API_KEY"):
  os.environ["GROQ_API_KEY"] = getpass.getpass("Enter API key for Groq: ")

llm = init_chat_model("llama3-8b-8192", model_provider="groq")

llm.invoke("hi")

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

prompt = ChatPromptTemplate.from_template(
"""Answer the question based only on the context provided.

Context: {context}

Question: {question}"""
)

def format_docs(docs):
    return "\n\n".join(f"text: {doc.page_content} metadata: {doc.metadata}" for doc in docs)

chain = (
    {"context": traversal_retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

chain.invoke("what animal could be found near a Lion?")

chain.invoke("what animal could be found near a cheetahs?")

chain.invoke("what animal can be found in Africa?")

chain.invoke("what all animal can be found in south America give me all the names?")

