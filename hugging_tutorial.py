# -*- coding: utf-8 -*-
"""hugging_tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11gtAClPmOGXztrnc5O8FugAgkmUyq604
"""

!pip install transformers

from transformers import pipeline

"""distilbert/distilbert-base-uncased-finetuned-sst-2-english

Stanford Sentiment Treebank-2




"""

classifier=pipeline("sentiment-analysis")

classifier("i will learn AI throughout my entire life it is like a passion")

classifier("my friend is evil")

generation=pipeline("text-generation")

generation("python is a simple language what is your thought")

generation("hello how are you?")

"""sshleifer/distilbart-cnn-12-6

repo_name/model_name
"""

summarizer=pipeline("summarization")

text="A large language model (LLM) is a machine learning model that uses deep learning to perform natural language processing (NLP) tasks. LLMs are trained on large amounts of text to learn how to respond to user requests with human-like language."

"""text="A large language model (LLM) is a machine learning model that uses deep learning to perform natural language processing (NLP) tasks. LLMs are trained on large amounts of text to learn how to respond to user requests with human-like language."

{'summary_text': ' A large language model (LLM) is a machine learning model that uses deep learning to perform natural language processing (NLP) tasks . LLMs are trained on large amounts of text to learn how to respond to user requests with human-like language . LLM is trained on . large amounts . of text .'}]
"""

print(summarizer(text,max_length=10))

classifier=pipeline("zero-shot-classification")
res = classifier(
    "This is a course about Python list comprehension",
    candidate_labels=["education", "politics", "business"],
)

print(res)

bert_summarizer=pipeline("summarization",model="facebook/bart-large-cnn")

text

print(bert_summarizer(text,max_new_tokens=20))

"""Use AutoModelForSequenceClassification and AutoTokenizer to load the pretrained model and itâ€™s associated tokenizer (more on an AutoClass in the next section):

Bert(pre_train)--> vocab
GPT(pre_traini)-->vocab

python-->split()

spacy

nltk

sent-->tokens-->embedding-->pe-->self_attention-->ff-->o/p
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer=AutoTokenizer.from_pretrained("google/flan-t5-large")

model=AutoModelForSequenceClassification.from_pretrained("google/flan-t5-large")

tokenizer

model

save_directory="my_model_dir"

tokenizer.save_pretrained(save_directory)

model.save_pretrained(save_directory)

classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer,device=0)

res = classifier("I've been waiting for a HuggingFace course my whole life.")

print(res)







"""1. huggingface use via langchain
2. finetune llm or lm model using hf transformer
3. TRL
4. text-->audio/text-->image
"""

!pip install langchain-huggingface

sequence = "Using a Transformer network is simple"

res = tokenizer(sequence)
print(res)

tokens = tokenizer.tokenize(sequence)
print(tokens)

ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)

decoded_string = tokenizer.decode(ids)
print(decoded_string)

