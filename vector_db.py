# -*- coding: utf-8 -*-
"""vector_db-18323819210bf01aaf6c9efe25d31940.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nC-BTi9KM9lk_Fqb1ABLLecSez1fSeMZ
"""

print("ok")

from langchain_community.document_loaders import PyPDFLoader

from langchain_community.document_loaders import PyPDFDirectoryLoader

directory_loader=PyPDFDirectoryLoader("C:\\Complete_Content\\GENERATIVEAI\\NEW_E2E_COURSE\\genai_bootcamp\\data")

len(directory_loader.load())

loader=PyPDFLoader(r"C:\\Complete_Content\\GENERATIVEAI\\NEW_E2E_COURSE\\genai_bootcamp\\data\\llama2.pdf")

dcouments=loader.load()

dcouments

len(dcouments)

dcouments[0].metadata

dcouments[0].page_content

for doc in dcouments:
    print(doc.page_content)
    print("##################################################")

from dotenv import load_dotenv
load_dotenv()

from langchain_openai import OpenAIEmbeddings

openai_embedding=OpenAIEmbeddings(model='text-embedding-3-large')

# #these all the methods are for chunking
# 1. character textsplitter(sepration based on given char)
# 2. character recursive text splitter(regex)
# 3. token text splitter(LLM capability)

import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS

dcouments[0].page_content

dim=len(openai_embedding.embed_query(dcouments[0].page_content))

faiss_index=faiss.IndexFlatIP(dim)

vector_store = FAISS(
    embedding_function=openai_embedding,
    index=faiss_index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={},
)

dcouments

vector_store.add_documents(dcouments)

vector_store.similarity_search(
    query="what is llama2 and what is a difference between llama2 and mistral?",
    k=1
)

